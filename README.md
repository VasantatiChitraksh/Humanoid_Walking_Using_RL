# Vision-Integrated Humanoid Locomotion using PPO

**Authors:** Chitraksh V, V Akhil, Nithish Goud S

This repository houses a comprehensive framework for training a MuJoCo Humanoid agent to achieve stable bipedal locomotion. By bridging **Computer Vision** with **Deep Reinforcement Learning (DRL)**, this project moves beyond random initialization. We implement a pipeline that extracts skeletal posture from a 2D image of a human and translates it into the 3D joint space of a robot, allowing the agent to "mimic" a starting pose before initiating its walking gait via Proximal Policy Optimization (PPO).

## üìÇ Project Architecture

Based on the file structure, the project is organized as follows:

```plaintext
Humanoid_lib/
‚îú‚îÄ‚îÄ checkpoints/             # Storage for model weights saved during training
‚îú‚îÄ‚îÄ lib/                     # Core RL implementation
‚îÇ   ‚îú‚îÄ‚îÄ DQN_Agent.py         # Agent class containing Actor-Critic networks
‚îÇ   ‚îú‚îÄ‚îÄ buffer_DQN.py        # Replay buffer for trajectory storage & GAE calculation
‚îÇ   ‚îî‚îÄ‚îÄ utils.py             # Environment wrappers and logging utilities
‚îú‚îÄ‚îÄ logs/                    # TensorBoard event logs
‚îú‚îÄ‚îÄ pose_estimation/         # Computer Vision Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ imageprocessing.py   # Image resizing and normalization
‚îÇ   ‚îú‚îÄ‚îÄ keypoints.py         # MediaPipe extraction & skeleton overlay logic
‚îÇ   ‚îî‚îÄ‚îÄ Kinematic.py         # Mathematics for 2D->3D joint angle conversion
‚îú‚îÄ‚îÄ videos/                  # Output renderings of the agent
‚îú‚îÄ‚îÄ get_joint_data.py        # Debug tool for inspecting Mujoco joint limits
‚îú‚îÄ‚îÄ test_ppo.py              # INFERENCE: Runs the visualizer with the trained model
‚îú‚îÄ‚îÄ train_ppo.py             # TRAINING: Starts the PPO training loop
‚îú‚îÄ‚îÄ requirements.txt                  # Dependency list
‚îî‚îÄ‚îÄ model.pt                 # Default pre-trained model
```

## üõ†Ô∏è Installation & Setup

**Prerequisites:** Python 3.12 is recommended to ensure compatibility with MediaPipe and PyTorch.

1.  **Clone and Navigate**
    Ensure you are in the root directory of the project (`Humanoid_lib`).

2.  **Environment Setup**

    ```bash
    # Create virtual environment
    python3.12 -m venv venv

    # Activate environment
    # Windows:
    .\venv\Scripts\activate
    # Mac/Linux:
    source venv/bin/activate
    ```

3.  **Install Dependencies**

    ```bash
    pip install -r req.txt
    ```

## üíª Usage Instructions

### 1\. Inference (Running the Demo)

To see the robot mimic a pose and walk using the pre-trained weights:

1.  Place your target image (e.g., `image.jpg`) in the root folder.
2.  Run the test script:
    ```bash
    python test_ppo.py
    ```
3.  Enter the image filename when prompted. The simulation window will launch in fullscreen.

### 2\. Training from Scratch

To train a new agent using the PPO algorithm:

1.  Execute the training script:
    ```bash
    python train_ppo.py
    ```
2.  **Optional Arguments:** You can modify hyperparameters directly in the script or add command-line parsers (e.g., `--n-epochs=2000`).
3.  **Monitoring:** Track loss metrics and reward curves via TensorBoard:
    ```bash
    tensorboard --logdir=logs
    ```

## üß† Technical Modules

### Module 1: Pose Estimation (Computer Vision)
- Accepts a 2D human image as input.
- Uses MediaPipe to extract the body keypoints.
- Generates a skeletal overlay on the image for visual debugging.
- Produces normalized 3D landmark coordinates using depth estimation and scaling.
- Passes the coordinates data to the kinematic mapping module.

### Module 2: Kinematic Mapping (Human to Robot Translation)
- Receives 3D body keypoints from Module 1.
- Computes inverse kinematic transformations to determine robot joint rotations.
- Outputs a 3D robot joint configuration used as the starting posture in simulation.

### Module 3: Reinforcement Learning (PPO Locomotion Control)
- Uses MuJoCo Humanoid-v5 environment with continuous action space.
- Implements PPO (Actor‚ÄìCritic) to control joint torques for locomotion.
- Reward encourages forward walking, balance, smooth actuation, and energy efficiency.
- Includes dynamic torso stabilization force to counter backward fall during early training.
- Agent starts from the posture generated by Module 2.

### Output
#### Module 1
![Module1](https://github.com/user-attachments/assets/ba89786e-b990-4570-bcbb-ec956ad825e8)

#### Module 2
Pose: ![Module2 Pose](https://github.com/user-attachments/assets/cd67f6dc-130d-4103-b2a3-793c5101c603)

Output: ![Module2 Robot](https://github.com/user-attachments/assets/e1fa85a7-0470-4764-98be-e818a82b2a06)

#### Module 3

Walking Robot: ![Module3_Robot](https://github.com/user-attachments/assets/dd9205bd-2f77-46b2-aa95-084001cec852)




#### Module 3
![Module3_Video](https://drive.google.com/file/d/1TD7cbYFNNDfeI3FOIVPN7zyb5eyQ5kyl/view?usp=drive_link)
